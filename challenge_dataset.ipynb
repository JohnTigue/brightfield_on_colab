{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "challenge_dataset.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQQb1m85EkvC",
        "colab_type": "text"
      },
      "source": [
        "# BARC dataset\n",
        "\n",
        "In many ways this BARC dataset is much like The Allen's [BigNeuron project](https://alleninstitute.org/what-we-do/brain-science/news-press/articles/bigneuron-project-launched-advance-3d-reconstructions-neurons) of a few years ago. \n",
        "\n",
        "Here though there is only one type of data: brightfield imaging of biocytin stained neurons. Each neuron's image stack on the order of 10 gigabytes of data.\n",
        "\n",
        "## Access info\n",
        "The challenge dataset is hosted on Wasabi Cloud Storage, which mimics the APIs of AWS S3 so all the regular ways of accessing data on S3 can be used to access the data\n",
        "\n",
        "- Service endpoint address: s3.wasabisys.com\n",
        "- Access Key Id: 2G7POM6IZKJ3KLHSC4JB\n",
        "- Secret Access Key: 0oHD5BXPim7fR1n7zDXpz4YoB7CHAHAvFgzpuJnt\n",
        "- Storage region: us-west-1\n",
        "- bucket name: brightfield-auto-reconstruction-competition  \n",
        "\n",
        "## Overview of bucket's contents\n",
        "There are two parts to the data\n",
        "1. training data (100+ neurons, with manual SWCs)\n",
        "2. test data (10 neurons, no SWCs)\n",
        "\n",
        "Each neuron is in its own folder off the root of the bucket. So the are over 100 folders with names like `647225829`, `767485082`, and `861519869`.\n",
        "\n",
        "Each neuron's data is in a separate folder. Each folder consists of\n",
        "- the input: a few hundred TIFF image files\n",
        "- the output: one SWC manually traced skeleton file\n",
        "\n",
        "There is one unusual sub-root folder, `TEST_DATA_SET`, which contains the data for the ten neurons used during the challenge's evaluation phase. These ten neuron image stacks *DO NOT* have SWC files.\n",
        "\n",
        "The goal is that some software will read the image stack  and auto reconstruct the SWC, without a human having to manually craft a SWC skeleton file (or at least minimize the human input time).\n",
        "\n",
        "So, the idea is a two phase challenge: first train with answers (SWC files), then submit 10 SWC files the program generates on the ten neurons in `TEST_DATA_SET`. \n",
        "\n",
        "sfirst train a auto reconstruction program using the roughly 100 neurons in the training data set, and check your results against the human traced SWC skeletons that each neuron's image stack comes with. Then for the evaluation phase\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Each image stack has its own image count, seemingly a few hunderd TIFF images each (e.g., 270, 500, 309, etc.). Each stack's images are all the same size but the sizes differ between stacks (e.g. 33MB images, 58MB images, etc.). Seemingly, on the order of 30 to 50 MB per image. \n",
        "\n",
        "One TEST_DATA_SET sample neuron's data is a folder, named `665856925`:\n",
        "- Full of about 280 TIFF images\n",
        "- All files named like:`reconstruction_0_0539044525_639962984-0007.tif` \n",
        "- The only thing that changes is the last four characters in the filename root, after the hyphen.\n",
        "- Each file is about 33 MB in size\n",
        "- One neuron's data is on the order of 10 gigabyte\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kGsnQdpN7Bk",
        "colab_type": "text"
      },
      "source": [
        "## Colab can handle one neuron's data at a time\n",
        "\n",
        "\n",
        "Consider one large neuron, Name/ID of `647225829`. This one has 460 images, each 57.7MB. So, a single neuron's data can be as big as, say, 25 gigabytes. \n",
        "\n",
        "Fortuneately, Google's Colab has that much file system. They give out 50GB file systems. And if you ask for a GPU they actually give you 350GB. (U-Net can use a GPU.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thLxqVkTEeBg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "cdfc3359-b0c5-4478-d8e9-a95f2ee681c0"
      },
      "source": [
        "# Get some stats on the file system:\n",
        "!!df -h .\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Filesystem      Size  Used Avail Use% Mounted on',\n",
              " 'overlay          49G   25G   22G  54% /']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMl6yoM8Y8RF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AF1wgPGPNjov",
        "colab_type": "text"
      },
      "source": [
        "The default file system on Colab is 50G, but a 360G file system can be requested, simply by configuring the runtime to have a GPU (yup).\n",
        "\n",
        "So, on the default (25G) file system, half the file system is already used by the OS and other pre-installed software. A big neuron's data would consume the remaining 25G. So **probably a good idea to request a GPU** which will also come with ~360G file system.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPPpOhoAZfo4",
        "colab_type": "text"
      },
      "source": [
        "## Overview of the dataset\n",
        "\n",
        "\n",
        "**The goal here** is to have a bit of utility code that completely maps the dataset's file system, programmatically walking the file system. \n",
        "\n",
        "All this tedius code makes two things:\n",
        "1. training_neurons: dictionary (105 neurons) keyed by neuron_id \n",
        "2. testing_neurons: dictionary (10 neurons) keyed by neuron_id\n",
        "\n",
        "All 115 neurons and all their files (names and sizes) programmatically indexed into a convenient data structure with which to build out manifest files for, say, ShuTu or some U-Net reconstructor to process. I.e. this will make it easier for folks to massage the data into whatever tool they decide to run with.\n",
        "\n",
        "The data is stored on Wasabi Cloud Storage, which mimics the AWS S3 APIs, so AWS's Python client, boto3, can be used to access the data. boto3 comes preinstalled on Colab. Here's Wasabi's how-to doc, [How do I use the AWS SDK for Python (boto3) with Wasabi?\n",
        "](https://wasabi-support.zendesk.com/hc/en-us/articles/115002579891-How-do-I-use-the-AWS-SDK-for-Python-boto3-with-Wasabi-)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akBdiF6snGGo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "99d2ddcc-d0d3-4579-b009-fb5d77b2027d"
      },
      "source": [
        "import boto3\n",
        "from IPython.display import HTML, display\n",
        "import time\n",
        "\n",
        "def progress(value, max=100):\n",
        "    return HTML(\"\"\"\n",
        "        <progress\n",
        "            value='{value}'\n",
        "            max='{max}',\n",
        "            style='width: 100%'\n",
        "        >\n",
        "            {value}\n",
        "        </progress>\n",
        "    \"\"\".format(value=value, max=max))\n",
        "\n",
        "# Tweaked out https://stackoverflow.com/a/49361727 or https://stackoverflow.com/a/14822210\n",
        "def format_bytes(size):\n",
        "    # 2**10 = 1024\n",
        "    power = 2**10\n",
        "    n = 0\n",
        "    power_labels = {0 : '', 1: 'K', 2: 'M', 3: 'G', 4: 'T'}\n",
        "    while size > power:\n",
        "        size /= power\n",
        "        n += 1\n",
        "    return size, power_labels[n]+'B'\n",
        "    \n",
        "def sumObjectsForPrefix(a_prefix):\n",
        "  \"sums gigabytes of file system occupied by all objects is a directory)\"\n",
        "  tots = 0\n",
        "  tots = sum(1 for _ in bucket.objects.filter(Prefix = a_prefix)) \n",
        "  return tots\n",
        "\n",
        "s3 = boto3.resource('s3',\n",
        "     endpoint_url = 'https://s3.us-west-1.wasabisys.com',\n",
        "     aws_access_key_id = '2G7POM6IZKJ3KLHSC4JB',\n",
        "     aws_secret_access_key = \"0oHD5BXPim7fR1n7zDXpz4YoB7CHAHAvFgzpuJnt\")  \n",
        "bucket = s3.Bucket('brightfield-auto-reconstruction-competition')\n",
        "\n",
        "result = bucket.meta.client.list_objects(Bucket=bucket.name,\n",
        "                                         Delimiter='/')\n",
        "print( \"Total root subfolders = \" + str(sum(1 for _ in result.get('CommonPrefixes') )) + \". Mapping training image stacks, one at a time...\")\n",
        "\n",
        "# Walk the dataset file system. First the 105 training TIFF stacks, with SWCs                    \n",
        "\n",
        "# Set up a progress indicator for this slow task:\n",
        "progressIndicator = display(progress(0, 100), display_id=True)\n",
        "progressIndicator_count = 0\n",
        "progressIndicator_end = 105\n",
        "\n",
        "training_neurons = {}\n",
        "for o in result.get('CommonPrefixes'):\n",
        "  progressIndicator_count += 1\n",
        "  progressIndicator.update(progress(progressIndicator_count, progressIndicator_end))\n",
        "  a_prefix = o.get('Prefix')\n",
        "  # 106 lines of random numbers: \n",
        "  #print(a_prefix)\n",
        "  \n",
        "  # Enumerate all files\n",
        "  # print(\"----------------\")\n",
        "  imagestack_bytes = 0\n",
        "  imagestack = []\n",
        "  swc_key = None\n",
        "  for s3_object in bucket.objects.filter(Prefix = a_prefix):\n",
        "    # print(s3_object.key + \"= \" + str(s3_object.size))\n",
        "    if not s3_object.key.endswith(\".swc\"):\n",
        "      if s3_object.key != a_prefix:\n",
        "        # if == it's the directory itself, not a file in it so ignore\n",
        "        imagestack.append(s3_object.key)\n",
        "        imagestack_bytes += s3_object.size\n",
        "    else:\n",
        "      swc_key = s3_object.key\n",
        "  \n",
        "  if a_prefix != \"TEST_DATA_SET/\":\n",
        "    training_neurons[a_prefix] = {\"prefix\": a_prefix, \"swc\": swc_key, \"imagestack\": imagestack, \"size\": imagestack_bytes}\n",
        "        \n",
        "print( \"# training neurons mapped: \" + str(len(training_neurons)))    \n",
        "\n",
        "#for a_neuron_name in training_neurons:\n",
        "#  a_neuron = training_neurons[a_neuron_name]\n",
        "#  fileSize, fileUnits = format_bytes(a_neuron[\"size\"])\n",
        "#  print(a_neuron_name + \": \" + str(len(a_neuron[\"imagestack\"])) + \" files = \" + '{:4.1f}'.format(fileSize) + \" \" + fileUnits )\n",
        "      \n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total root subfolders = 106. Mapping training image stacks, one at a time...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "        <progress\n",
              "            value='106'\n",
              "            max='105',\n",
              "            style='width: 100%'\n",
              "        >\n",
              "            106\n",
              "        </progress>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "# training neurons mapped: 105\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDp7BKQjoItg",
        "colab_type": "text"
      },
      "source": [
        "106 folders for 105 training neurons and the last folder is `TEST_DATA_SET` which contains 10 neuron image stacks in subfolders (without SWC answers).\n",
        "\n",
        "\n",
        "Whelp, time and space are limited on Colab so let's figure out which neurons are the smallest ergo the fasted to process (hopefully).\n",
        "\n",
        "Sort the 105 training neurons by file size:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qotpVm7-ZIa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b6197fb2-ea06-45d6-d868-6ce10b4ad75f"
      },
      "source": [
        "def sizer(x): \n",
        "  return training_neurons[x][\"size\"]\n",
        "\n",
        "size_sorted = sorted(training_neurons, key = sizer) \n",
        "    \n",
        "for a_neuron_name in size_sorted:\n",
        "  a_neuron = training_neurons[a_neuron_name]\n",
        "  fileSize, fileUnits = format_bytes(a_neuron[\"size\"])\n",
        "  print(a_neuron_name + \": \" + str(len(a_neuron[\"imagestack\"])) + \" files = \" + '{:4.1f}'.format(fileSize) + \" \" + fileUnits )\n",
        "      \n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "651806289/: 291 files =  6.0 GB\n",
            "647289876/: 228 files =  7.0 GB\n",
            "651748297/: 336 files =  7.0 GB\n",
            "647244741/: 261 files =  8.0 GB\n",
            "713686035/: 289 files =  8.9 GB\n",
            "647247980/: 299 files =  9.1 GB\n",
            "649052017/: 307 files =  9.4 GB\n",
            "650917845/: 245 files = 10.0 GB\n",
            "672278613/: 330 files = 10.1 GB\n",
            "654221379/: 334 files = 10.3 GB\n",
            "676633030/: 387 files = 10.6 GB\n",
            "726555942/: 377 files = 11.6 GB\n",
            "706002308/: 378 files = 11.6 GB\n",
            "664466860/: 382 files = 11.7 GB\n",
            "739291676/: 386 files = 11.9 GB\n",
            "699207642/: 389 files = 11.9 GB\n",
            "669371214/: 295 files = 12.0 GB\n",
            "654591451/: 300 files = 12.2 GB\n",
            "836350796/: 413 files = 12.7 GB\n",
            "651511374/: 414 files = 12.7 GB\n",
            "729522604/: 431 files = 13.2 GB\n",
            "696228200/: 435 files = 13.4 GB\n",
            "651790667/: 250 files = 13.4 GB\n",
            "728251151/: 267 files = 14.2 GB\n",
            "668664690/: 464 files = 14.3 GB\n",
            "651834134/: 469 files = 14.4 GB\n",
            "673066511/: 283 files = 14.5 GB\n",
            "652113069/: 359 files = 14.6 GB\n",
            "821560343/: 361 files = 14.7 GB\n",
            "715286106/: 482 files = 14.8 GB\n",
            "739383450/: 506 files = 15.6 GB\n",
            "693978543/: 386 files = 15.7 GB\n",
            "724316403/: 387 files = 15.7 GB\n",
            "720948812/: 526 files = 16.1 GB\n",
            "712951287/: 527 files = 16.2 GB\n",
            "777467421/: 535 files = 16.4 GB\n",
            "691329423/: 538 files = 16.5 GB\n",
            "743214898/: 549 files = 16.8 GB\n",
            "663961066/: 414 files = 16.8 GB\n",
            "713016653/: 554 files = 17.0 GB\n",
            "692932326/: 557 files = 17.2 GB\n",
            "715328776/: 322 files = 17.2 GB\n",
            "744609566/: 426 files = 17.3 GB\n",
            "766985763/: 568 files = 17.4 GB\n",
            "647278927/: 346 files = 17.5 GB\n",
            "762912832/: 431 files = 17.5 GB\n",
            "718987297/: 444 files = 18.0 GB\n",
            "677347027/: 586 files = 18.0 GB\n",
            "687702530/: 358 files = 18.1 GB\n",
            "694613686/: 446 files = 18.1 GB\n",
            "719458528/: 341 files = 18.3 GB\n",
            "720463180/: 599 files = 18.4 GB\n",
            "674317065/: 344 files = 18.4 GB\n",
            "710114253/: 466 files = 18.9 GB\n",
            "861519869/: 468 files = 19.1 GB\n",
            "688712523/: 481 files = 19.6 GB\n",
            "797376860/: 485 files = 19.7 GB\n",
            "685884456/: 492 files = 20.1 GB\n",
            "777472440/: 519 files = 21.0 GB\n",
            "693441787/: 316 files = 21.1 GB\n",
            "832210870/: 402 files = 21.6 GB\n",
            "722603466/: 401 files = 22.4 GB\n",
            "767485082/: 444 files = 22.4 GB\n",
            "722033195/: 562 files = 22.8 GB\n",
            "774495631/: 563 files = 22.8 GB\n",
            "736979905/: 430 files = 23.1 GB\n",
            "706065773/: 572 files = 23.1 GB\n",
            "743274987/: 591 files = 24.0 GB\n",
            "677326176/: 595 files = 24.1 GB\n",
            "762275581/: 447 files = 24.2 GB\n",
            "757721211/: 603 files = 24.6 GB\n",
            "691830341/: 607 files = 24.7 GB\n",
            "768977785/: 609 files = 24.7 GB\n",
            "647225829/: 460 files = 24.7 GB\n",
            "704338365/: 614 files = 24.9 GB\n",
            "743918700/: 467 files = 25.1 GB\n",
            "712977942/: 622 files = 25.2 GB\n",
            "696560235/: 509 files = 25.7 GB\n",
            "815877776/: 479 files = 25.7 GB\n",
            "798631918/: 480 files = 25.9 GB\n",
            "694569649/: 486 files = 26.1 GB\n",
            "710124691/: 488 files = 26.2 GB\n",
            "663523681/: 539 files = 27.3 GB\n",
            "707517873/: 547 files = 27.6 GB\n",
            "689485972/: 362 files = 30.1 GB\n",
            "742421390/: 562 files = 30.2 GB\n",
            "745145893/: 567 files = 30.5 GB\n",
            "765078615/: 461 files = 30.8 GB\n",
            "721065710/: 589 files = 31.5 GB\n",
            "718706617/: 591 files = 31.6 GB\n",
            "715273626/: 594 files = 31.8 GB\n",
            "718476684/: 611 files = 32.7 GB\n",
            "704353262/: 612 files = 32.8 GB\n",
            "704363712/: 610 files = 32.9 GB\n",
            "702233284/: 614 files = 33.2 GB\n",
            "651829339/: 529 files = 35.3 GB\n",
            "741428906/: 591 files = 39.4 GB\n",
            "726635182/: 610 files = 40.7 GB\n",
            "772239618/: 617 files = 41.2 GB\n",
            "818150510/: 444 files = 44.1 GB\n",
            "845142280/: 535 files = 44.3 GB\n",
            "728203498/: 675 files = 45.0 GB\n",
            "697851947/: 850 files = 45.7 GB\n",
            "699189400/: 650 files = 53.8 GB\n",
            "687746742/: 608 files = 59.9 GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mOIXLrk_-oS",
        "colab_type": "text"
      },
      "source": [
        "In other words, there are 7 image stacks which are smaller than 10 GB. Those are:\n",
        "```\n",
        "651806289/: 291 files =  6.0 GB\n",
        "647289876/: 228 files =  7.0 GB\n",
        "651748297/: 336 files =  7.0 GB\n",
        "647244741/: 261 files =  8.0 GB\n",
        "713686035/: 289 files =  8.9 GB\n",
        "647247980/: 299 files =  9.1 GB\n",
        "649052017/: 307 files =  9.4 GB\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-l_U8XU_P2x",
        "colab_type": "text"
      },
      "source": [
        "### Test data set\n",
        "\n",
        "    \n",
        "# The ten training TIFF stacks, without SWCs                    \n",
        "The final part of the challenge data set to be mapped is the sub-root directory, `TEST_DATA_SET`, which has 10 neurons laid out like with the training data, except the SWC files are missing i.e. no reconstruction answers given, that is what the challenger is supposed to demonstrate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bp0cc2ixHx73",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "4773eaa2-3d17-4160-c05b-7dfdfa4bd0d5"
      },
      "source": [
        "import boto3\n",
        "\n",
        "client = boto3.client('s3',\n",
        "     endpoint_url = 'https://s3.us-west-1.wasabisys.com',\n",
        "     aws_access_key_id = '2G7POM6IZKJ3KLHSC4JB',\n",
        "     aws_secret_access_key = \"0oHD5BXPim7fR1n7zDXpz4YoB7CHAHAvFgzpuJnt\")\n",
        "paginator = client.get_paginator('list_objects')\n",
        "result = paginator.paginate(\n",
        "    Bucket='brightfield-auto-reconstruction-competition', \n",
        "    Prefix=\"TEST_DATA_SET/\", \n",
        "    Delimiter='/')\n",
        "    # See https://stackoverflow.com/a/36992023\n",
        "    # A response can contain CommonPrefixes only if you specify a delimiter. When you do, CommonPrefixes contains all (if there are any) keys between Prefix and the next occurrence of the string specified by delimiter. In effect, CommonPrefixes lists keys that act like subdirectories in the directory specified by Prefix.\n",
        "\n",
        "#for prefix in result.search('CommonPrefixes'):\n",
        "#    print(prefix.get('Prefix'))\n",
        "    \n",
        "testing_neurons = {}\n",
        "\n",
        "# Set up a progress indicator for this slow but not too slow task:\n",
        "progressIndicator = display(progress(0, 10), display_id=True)\n",
        "progressIndicator_count = 0\n",
        "progressIndicator_end = 10\n",
        "\n",
        "for o in result.search('CommonPrefixes'):\n",
        "  progressIndicator_count += 1\n",
        "  progressIndicator.update(progress(progressIndicator_count, progressIndicator_end))\n",
        "  a_prefix = o.get(\"Prefix\")\n",
        "  print(a_prefix)\n",
        "  \n",
        "  # Enumerate all files\n",
        "  # print(\"----------------\")\n",
        "  imagestack_bytes = 0\n",
        "  imagestack = []\n",
        "  swc_key = None\n",
        "  for s3_object in bucket.objects.filter(Prefix = a_prefix):\n",
        "    # print(s3_object.key + \"= \" + str(s3_object.size))\n",
        "    if not s3_object.key.endswith(\".swc\"):\n",
        "      if s3_object.key != a_prefix:\n",
        "        # if == it's the directory itself, not a file in it so ignore\n",
        "        imagestack.append(s3_object.key)\n",
        "        imagestack_bytes += s3_object.size\n",
        "    else:\n",
        "      swc_key = s3_object.key\n",
        "  \n",
        "  # Strip the \"TEST_DATA_SET/\" from Prefix\n",
        "  neuron_id = a_prefix[len(\"TEST_DATA_SET/\"):]\n",
        "  \n",
        "  testing_neurons[neuron_id] = {\"prefix\": a_prefix, \"swc\": swc_key, \"imagestack\": imagestack, \"size\": imagestack_bytes}\n",
        "        \n",
        "print( \"# testing neurons mapped: \" + str(len(testing_neurons)) + \"\\nSorted by size of image stack:\")    \n",
        "    \n",
        "def testing_sizer(x): \n",
        "  return testing_neurons[x][\"size\"]\n",
        "\n",
        "size_sorted_testing_neurons = sorted(testing_neurons, key = testing_sizer) \n",
        "    \n",
        "for a_neuron_name in size_sorted_testing_neurons:\n",
        "  a_neuron = testing_neurons[a_neuron_name]\n",
        "  fileSize, fileUnits = format_bytes(a_neuron[\"size\"])\n",
        "  print(a_neuron_name + \": \" + str(len(a_neuron[\"imagestack\"])) + \" files = \" + '{:4.1f}'.format(fileSize) + \" \" + fileUnits )\n",
        "  "
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "        <progress\n",
              "            value='10'\n",
              "            max='10',\n",
              "            style='width: 100%'\n",
              "        >\n",
              "            10\n",
              "        </progress>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "TEST_DATA_SET/665856925/\n",
            "TEST_DATA_SET/687730329/\n",
            "TEST_DATA_SET/691311995/\n",
            "TEST_DATA_SET/715953708/\n",
            "TEST_DATA_SET/741428906/\n",
            "TEST_DATA_SET/751017870/\n",
            "TEST_DATA_SET/761936495/\n",
            "TEST_DATA_SET/827413048/\n",
            "TEST_DATA_SET/850675694/\n",
            "TEST_DATA_SET/878858275/\n",
            "# testing neurons mapped: 10\n",
            "Sorted by size of image stack:\n",
            "665856925/: 281 files =  8.6 GB\n",
            "715953708/: 340 files = 10.4 GB\n",
            "751017870/: 465 files = 18.9 GB\n",
            "687730329/: 497 files = 20.3 GB\n",
            "850675694/: 438 files = 23.5 GB\n",
            "827413048/: 424 files = 28.3 GB\n",
            "761936495/: 529 files = 28.5 GB\n",
            "691311995/: 441 files = 29.4 GB\n",
            "741428906/: 591 files = 39.4 GB\n",
            "878858275/: 541 files = 54.0 GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Na2LMDvToDG9",
        "colab_type": "text"
      },
      "source": [
        "## Trial and error hacking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bIC5A8fuZEd",
        "colab_type": "text"
      },
      "source": [
        "### Progress bars on Colab\n",
        "\n",
        "For the progress bar code, see [SO and Colab thread](https://colab.research.google.com/drive/1I2o3Ie34vJ3G4M6eE54-OyrmzJNBwhOp#scrollTo=EbF9oPhzOqZj&forceEdit=true&sandboxMode=true)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCw8h3eMu2cZ",
        "colab_type": "text"
      },
      "source": [
        "### Probing S3/Wasabi\n",
        "\n",
        "Just kicking boto3 around. That's AWS code inspecting data on Wasabi, code running on Google's Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bg0FkqTaZlO8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import boto3\n",
        "\n",
        "\n",
        "s3 = boto3.resource('s3',\n",
        "     endpoint_url = 'https://s3.us-west-1.wasabisys.com',\n",
        "     aws_access_key_id = '2G7POM6IZKJ3KLHSC4JB',\n",
        "     aws_secret_access_key = '0oHD5BXPim7fR1n7zDXpz4YoB7CHAHAvFgzpuJnt')  \n",
        "  \n",
        "bucket = s3.Bucket('brightfield-auto-reconstruction-competition')\n",
        "#for obj in bucket.objects.filter(Prefix=\"668664690\"):\n",
        "#  print(obj.key)\n",
        "\n",
        "# Good:\n",
        "#print( \"objects=\" + str(sum(1 for _ in bucket.objects.filter(Prefix=\"668664690\"))))\n",
        "\n",
        "  \n",
        "#size = sum(1 for _ in bucket.objects.all())  \n",
        "  \n",
        "  \n",
        "#print ('sub-folders:')  \n",
        "#for obj in bucket.objects.filter(Delimiter=\"/\"):\n",
        "#   print(obj.key)\n",
        "\n",
        "\n",
        "print( \"objects=\" + str(sum(1 for _ in bucket.objects.filter(Delimiter=\"/\"))))\n",
        "\n",
        "  \n",
        "  \n",
        "result = bucket.meta.client.list_objects(Bucket=bucket.name,\n",
        "                                         Delimiter='/')\n",
        "for o in result.get('CommonPrefixes'):\n",
        "    print(o.get('Prefix'))  \n",
        "\n",
        "#list(prefix='/', delimiter='/')\n",
        "\n",
        "# long list of all files\n",
        "#for a_bucket_object in bucket.objects.all():\n",
        "#    print(a_bucket_object.key)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}