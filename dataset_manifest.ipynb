{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dataset_manifest.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCMEGDO9DjkX",
        "colab_type": "text"
      },
      "source": [
        "# Challenge dataset overview on file system level\n",
        "\n",
        "For more on the project's context, see [brightfield neuron reconstruction challenge](https://colab.research.google.com/drive/1qvwT-SxHpZSLQ88VeIOkR296pbZfQqTK).\n",
        "\n",
        "This notebook provides a high level overview tour of the files in the challenge dataset. The dataset is about 2.5 terabyte of data. This notebook looks at the dataset as random files, nothing about what is in the files\n",
        "\n",
        "The next notebook, [initial_dataset_visualization.ipynb](https://colab.research.google.com/drive/1ZxzDwD1UdYqhuTxckPLiOUYHin0MZmFQ#scrollTo=TaREcuFG6SSQ), actually visualizes the dataset on a digital microscopy level (read: show images), by deep diving on a single specimen cell's data (image stack and SWC skeleton).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQQb1m85EkvC",
        "colab_type": "text"
      },
      "source": [
        "## Access info\n",
        "The challenge dataset is hosted on Wasabi Cloud Storage, which mimics the APIs of AWS S3 so all the regular ways of accessing data on S3 can be used to access the data\n",
        "\n",
        "- Service endpoint address: s3.wasabisys.com\n",
        "- Access Key Id: 2G7POM6IZKJ3KLHSC4JB\n",
        "- Secret Access Key: 0oHD5BXPim7fR1n7zDXpz4YoB7CHAHAvFgzpuJnt\n",
        "- Storage region: us-west-1\n",
        "- bucket name: brightfield-auto-reconstruction-competition  \n",
        "\n",
        "## Overview of bucket's contents\n",
        "There are two parts to the data\n",
        "1. Training data (105 neurons, with manual SWCs): 2.2 TB\n",
        "2. Test data (10 neurons, no SWCs): 261.3 GB\n",
        "\n",
        "Each neuron is in its own folder off the root of the bucket. So the are over 100 folders with names like `647225829`, `767485082`, and `861519869`.\n",
        "\n",
        "Each neuron's data is in a separate folder. Each folder consists of\n",
        "- the input: a few hundred TIFF image files\n",
        "- the output: one SWC manually traced skeleton file\n",
        "\n",
        "There is one unusual sub-root folder, `TEST_DATA_SET`, which contains the data for the ten neurons used during the challenge's evaluation phase. These ten neuron image stacks *DO NOT* have SWC files.\n",
        "\n",
        "The goal is that some software will read the image stack  and auto reconstruct the SWC, without a human having to manually craft a SWC skeleton file (or at least minimize the human input time).\n",
        "\n",
        "So, the idea is a two phase challenge: first train with answers (SWC files), then submit 10 SWC files the program generates on the ten neurons in `TEST_DATA_SET`. \n",
        "\n",
        "sfirst train a auto reconstruction program using the roughly 100 neurons in the training data set, and check your results against the human traced SWC skeletons that each neuron's image stack comes with. Then for the evaluation phase\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Each image stack has its own image count, seemingly a few hunderd TIFF images each (e.g., 270, 500, 309, etc.). Each stack's images are all the same size but the sizes differ between stacks (e.g. 33MB images, 58MB images, etc.). Seemingly, on the order of 30 to 50 MB per image. \n",
        "\n",
        "One TEST_DATA_SET sample neuron's data is a folder, named `665856925`:\n",
        "- Full of about 280 TIFF images\n",
        "- All files named like:`reconstruction_0_0539044525_639962984-0007.tif` \n",
        "- The only thing that changes is the last four characters in the filename root, after the hyphen.\n",
        "- Each file is about 33 MB in size\n",
        "- One neuron's data is on the order of 10 gigabyte\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kGsnQdpN7Bk",
        "colab_type": "text"
      },
      "source": [
        "## Colab can handle one neuron's data at a time\n",
        "\n",
        "\n",
        "Consider one large neuron, Name/ID of `647225829`. This one has 460 images, each 57.7MB. So, an average neuron's data can be as big as, say, 25 gigabytes. They range from ~6GB to ~60GB (specimen 687746742 is 59.9GB)\n",
        "\n",
        "Fortuneately, Google's Colab has that much file system. They give out 50GB file systems by default. And if you ask for a GPU they actually give you 350GB. \n",
        "\n",
        "350GB is enough file system to process the largest specimen in the dataset. Additionally, the U-Net implementation can use the T4 GPU. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thLxqVkTEeBg",
        "colab_type": "code",
        "outputId": "7ec81c04-bf00-4e97-9abf-8f669f94b379",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Get some stats on the file system:\n",
        "!!df -h .\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Filesystem      Size  Used Avail Use% Mounted on',\n",
              " 'overlay          49G   25G   22G  54% /']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMl6yoM8Y8RF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AF1wgPGPNjov",
        "colab_type": "text"
      },
      "source": [
        "The default file system on Colab is 50G, but a 360G file system can be requested, simply by configuring the runtime to have a GPU (yup).\n",
        "\n",
        "So, on the default (25G) file system, half the file system is already used by the OS and other pre-installed software. A big neuron's data would consume the remaining 25G. So **probably a good idea to request a GPU** which will also come with ~360G file system.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPPpOhoAZfo4",
        "colab_type": "text"
      },
      "source": [
        "## Overview of the dataset\n",
        "\n",
        "\n",
        "**The goal here** is to have a bit of utility code that completely maps the dataset's file system, programmatically walking the file system. \n",
        "\n",
        "All this tedius code makes two things:\n",
        "1. training_neurons: dictionary (105 neurons) keyed by neuron_id \n",
        "2. testing_neurons: dictionary (10 neurons) keyed by neuron_id\n",
        "\n",
        "All 115 neurons and all their files (names and sizes) programmatically indexed into a convenient data structure with which to build out manifest files for, say, ShuTu or some U-Net reconstructor to process. I.e. this will make it easier for folks to massage the data into whatever tool they decide to run with.\n",
        "\n",
        "The data is stored on Wasabi Cloud Storage, which mimics the AWS S3 APIs, so AWS's Python client, boto3, can be used to access the data. boto3 comes preinstalled on Colab. Here's Wasabi's how-to doc, [How do I use the AWS SDK for Python (boto3) with Wasabi?\n",
        "](https://wasabi-support.zendesk.com/hc/en-us/articles/115002579891-How-do-I-use-the-AWS-SDK-for-Python-boto3-with-Wasabi-)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYmCynK-Qbi4",
        "colab_type": "text"
      },
      "source": [
        "### Map the 105 training data neurons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akBdiF6snGGo",
        "colab_type": "code",
        "outputId": "8f9a583f-69d5-4e25-c59f-11c3b2ac612d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "import boto3\n",
        "from IPython.display import HTML, display\n",
        "import time\n",
        "\n",
        "def progress(value, max=100):\n",
        "    return HTML(\"\"\"\n",
        "        <progress\n",
        "            value='{value}'\n",
        "            max='{max}',\n",
        "            style='width: 100%'\n",
        "        >\n",
        "            {value}\n",
        "        </progress>\n",
        "    \"\"\".format(value=value, max=max))\n",
        "\n",
        "# Tweaked out https://stackoverflow.com/a/49361727 or https://stackoverflow.com/a/14822210\n",
        "def format_bytes(size):\n",
        "    # 2**10 = 1024\n",
        "    power = 2**10\n",
        "    n = 0\n",
        "    power_labels = {0 : '', 1: 'K', 2: 'M', 3: 'G', 4: 'T'}\n",
        "    while size > power:\n",
        "        size /= power\n",
        "        n += 1\n",
        "    return size, power_labels[n]+'B'\n",
        "    \n",
        "def sumObjectsForPrefix(a_prefix):\n",
        "  \"sums gigabytes of file system occupied by all objects is a directory)\"\n",
        "  tots = 0\n",
        "  tots = sum(1 for _ in bucket.objects.filter(Prefix = a_prefix)) \n",
        "  return tots\n",
        "\n",
        "s3 = boto3.resource('s3',\n",
        "     endpoint_url = 'https://s3.us-west-1.wasabisys.com',\n",
        "     aws_access_key_id = '2G7POM6IZKJ3KLHSC4JB',\n",
        "     aws_secret_access_key = \"0oHD5BXPim7fR1n7zDXpz4YoB7CHAHAvFgzpuJnt\")  \n",
        "bucket = s3.Bucket('brightfield-auto-reconstruction-competition')\n",
        "\n",
        "result = bucket.meta.client.list_objects(Bucket=bucket.name,\n",
        "                                         Delimiter='/')\n",
        "print( \"Total root subfolders = \" + str(sum(1 for _ in result.get('CommonPrefixes') )) + \". Mapping training image stacks, one at a time...\")\n",
        "\n",
        "# Walk the dataset file system. First the 105 training TIFF stacks, with SWCs                    \n",
        "\n",
        "# Set up a progress indicator for this slow task:\n",
        "progressIndicator = display(progress(0, 100), display_id=True)\n",
        "progressIndicator_count = 0\n",
        "progressIndicator_end = 105\n",
        "\n",
        "training_neurons = {}\n",
        "for o in result.get('CommonPrefixes'):\n",
        "  progressIndicator_count += 1\n",
        "  progressIndicator.update(progress(progressIndicator_count, progressIndicator_end))\n",
        "  a_prefix = o.get('Prefix')\n",
        "  # 106 lines of random numbers: \n",
        "  #print(a_prefix)\n",
        "  \n",
        "  # Enumerate all files\n",
        "  # print(\"----------------\")\n",
        "  imagestack_bytes = 0\n",
        "  imagestack = []\n",
        "  swc_key = None\n",
        "  for s3_object in bucket.objects.filter(Prefix = a_prefix):\n",
        "    # print(s3_object.key + \"= \" + str(s3_object.size))\n",
        "    if not s3_object.key.endswith(\".swc\"):\n",
        "      if s3_object.key != a_prefix:\n",
        "        # if == it's the directory itself, not a file in it so ignore\n",
        "        imagestack.append(s3_object.key)\n",
        "        imagestack_bytes += s3_object.size\n",
        "    else:\n",
        "      swc_key = s3_object.key\n",
        "  \n",
        "  if a_prefix != \"TEST_DATA_SET/\":\n",
        "    training_neurons[a_prefix] = {\"prefix\": a_prefix, \"swc\": swc_key, \"imagestack\": imagestack, \"size\": imagestack_bytes}\n",
        "        \n",
        "print( \"# training neurons mapped: \" + str(len(training_neurons)))    \n",
        "\n",
        "#for a_neuron_name in training_neurons:\n",
        "#  a_neuron = training_neurons[a_neuron_name]\n",
        "#  fileSize, fileUnits = format_bytes(a_neuron[\"size\"])\n",
        "#  print(a_neuron_name + \": \" + str(len(a_neuron[\"imagestack\"])) + \" files = \" + '{:4.1f}'.format(fileSize) + \" \" + fileUnits )\n",
        "      \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total root subfolders = 106. Mapping training image stacks, one at a time...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "        <progress\n",
              "            value='106'\n",
              "            max='105',\n",
              "            style='width: 100%'\n",
              "        >\n",
              "            106\n",
              "        </progress>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "# training neurons mapped: 105\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDp7BKQjoItg",
        "colab_type": "text"
      },
      "source": [
        "106 folders for 105 training neurons and the last folder is `TEST_DATA_SET` which contains 10 neuron image stacks in subfolders (without SWC answers).\n",
        "\n",
        "\n",
        "Whelp, time and space are limited on Colab so let's figure out which neurons are the smallest ergo the fasted to process (hopefully).\n",
        "\n",
        "Sort the 105 training neurons by file size:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qotpVm7-ZIa",
        "colab_type": "code",
        "outputId": "20aecf39-4959-41be-c673-3191e66be7fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def sizer(x): \n",
        "  return training_neurons[x][\"size\"]\n",
        "\n",
        "size_sorted = sorted(training_neurons, key = sizer) \n",
        "total_bytes_in_training_dataset = 0    \n",
        "  \n",
        "for a_neuron_name in size_sorted:\n",
        "  a_neuron = training_neurons[a_neuron_name]\n",
        "  fileSize, fileUnits = format_bytes(a_neuron[\"size\"])\n",
        "  total_bytes_in_training_dataset += a_neuron[\"size\"]\n",
        "  print(a_neuron_name + \": \" + str(len(a_neuron[\"imagestack\"])) + \" files = \" + '{:4.1f}'.format(fileSize) + \" \" + fileUnits )\n",
        "\n",
        "fileSize, fileUnits = format_bytes(total_bytes_in_training_dataset)\n",
        "print(\"\\nTotal size of training dataset = \" + '{:4.1f}'.format(fileSize) + \" \" + fileUnits )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "651806289/: 291 files =  6.0 GB\n",
            "647289876/: 228 files =  7.0 GB\n",
            "651748297/: 336 files =  7.0 GB\n",
            "647244741/: 261 files =  8.0 GB\n",
            "713686035/: 289 files =  8.9 GB\n",
            "647247980/: 299 files =  9.1 GB\n",
            "649052017/: 307 files =  9.4 GB\n",
            "650917845/: 245 files = 10.0 GB\n",
            "672278613/: 330 files = 10.1 GB\n",
            "654221379/: 334 files = 10.3 GB\n",
            "676633030/: 387 files = 10.6 GB\n",
            "726555942/: 377 files = 11.6 GB\n",
            "706002308/: 378 files = 11.6 GB\n",
            "664466860/: 382 files = 11.7 GB\n",
            "739291676/: 386 files = 11.9 GB\n",
            "699207642/: 389 files = 11.9 GB\n",
            "669371214/: 295 files = 12.0 GB\n",
            "654591451/: 300 files = 12.2 GB\n",
            "836350796/: 413 files = 12.7 GB\n",
            "651511374/: 414 files = 12.7 GB\n",
            "729522604/: 431 files = 13.2 GB\n",
            "696228200/: 435 files = 13.4 GB\n",
            "651790667/: 250 files = 13.4 GB\n",
            "728251151/: 267 files = 14.2 GB\n",
            "668664690/: 464 files = 14.3 GB\n",
            "651834134/: 469 files = 14.4 GB\n",
            "673066511/: 283 files = 14.5 GB\n",
            "652113069/: 359 files = 14.6 GB\n",
            "821560343/: 361 files = 14.7 GB\n",
            "715286106/: 482 files = 14.8 GB\n",
            "739383450/: 506 files = 15.6 GB\n",
            "693978543/: 386 files = 15.7 GB\n",
            "724316403/: 387 files = 15.7 GB\n",
            "720948812/: 526 files = 16.1 GB\n",
            "712951287/: 527 files = 16.2 GB\n",
            "777467421/: 535 files = 16.4 GB\n",
            "691329423/: 538 files = 16.5 GB\n",
            "743214898/: 549 files = 16.8 GB\n",
            "663961066/: 414 files = 16.8 GB\n",
            "713016653/: 554 files = 17.0 GB\n",
            "692932326/: 557 files = 17.2 GB\n",
            "715328776/: 322 files = 17.2 GB\n",
            "744609566/: 426 files = 17.3 GB\n",
            "766985763/: 568 files = 17.4 GB\n",
            "647278927/: 346 files = 17.5 GB\n",
            "762912832/: 431 files = 17.5 GB\n",
            "718987297/: 444 files = 18.0 GB\n",
            "677347027/: 586 files = 18.0 GB\n",
            "687702530/: 358 files = 18.1 GB\n",
            "694613686/: 446 files = 18.1 GB\n",
            "719458528/: 341 files = 18.3 GB\n",
            "720463180/: 599 files = 18.4 GB\n",
            "674317065/: 344 files = 18.4 GB\n",
            "710114253/: 466 files = 18.9 GB\n",
            "861519869/: 468 files = 19.1 GB\n",
            "688712523/: 481 files = 19.6 GB\n",
            "797376860/: 485 files = 19.7 GB\n",
            "685884456/: 492 files = 20.1 GB\n",
            "777472440/: 519 files = 21.0 GB\n",
            "693441787/: 316 files = 21.1 GB\n",
            "832210870/: 402 files = 21.6 GB\n",
            "722603466/: 401 files = 22.4 GB\n",
            "767485082/: 444 files = 22.4 GB\n",
            "722033195/: 562 files = 22.8 GB\n",
            "774495631/: 563 files = 22.8 GB\n",
            "736979905/: 430 files = 23.1 GB\n",
            "706065773/: 572 files = 23.1 GB\n",
            "743274987/: 591 files = 24.0 GB\n",
            "677326176/: 595 files = 24.1 GB\n",
            "762275581/: 447 files = 24.2 GB\n",
            "757721211/: 603 files = 24.6 GB\n",
            "691830341/: 607 files = 24.7 GB\n",
            "768977785/: 609 files = 24.7 GB\n",
            "647225829/: 460 files = 24.7 GB\n",
            "704338365/: 614 files = 24.9 GB\n",
            "743918700/: 467 files = 25.1 GB\n",
            "712977942/: 622 files = 25.2 GB\n",
            "696560235/: 509 files = 25.7 GB\n",
            "815877776/: 479 files = 25.7 GB\n",
            "798631918/: 480 files = 25.9 GB\n",
            "694569649/: 486 files = 26.1 GB\n",
            "710124691/: 488 files = 26.2 GB\n",
            "663523681/: 539 files = 27.3 GB\n",
            "707517873/: 547 files = 27.6 GB\n",
            "689485972/: 362 files = 30.1 GB\n",
            "742421390/: 562 files = 30.2 GB\n",
            "745145893/: 567 files = 30.5 GB\n",
            "765078615/: 461 files = 30.8 GB\n",
            "721065710/: 589 files = 31.5 GB\n",
            "718706617/: 591 files = 31.6 GB\n",
            "715273626/: 594 files = 31.8 GB\n",
            "718476684/: 611 files = 32.7 GB\n",
            "704353262/: 612 files = 32.8 GB\n",
            "704363712/: 610 files = 32.9 GB\n",
            "702233284/: 614 files = 33.2 GB\n",
            "651829339/: 529 files = 35.3 GB\n",
            "741428906/: 591 files = 39.4 GB\n",
            "726635182/: 610 files = 40.7 GB\n",
            "772239618/: 617 files = 41.2 GB\n",
            "818150510/: 444 files = 44.1 GB\n",
            "845142280/: 535 files = 44.3 GB\n",
            "728203498/: 675 files = 45.0 GB\n",
            "697851947/: 850 files = 45.7 GB\n",
            "699189400/: 650 files = 53.8 GB\n",
            "687746742/: 608 files = 59.9 GB\n",
            "\n",
            "Total size of training dataset =  2.2 TB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mOIXLrk_-oS",
        "colab_type": "text"
      },
      "source": [
        "In other words, there are 7 image stacks which are smaller than 10 GB. Those are:\n",
        "```\n",
        "651806289/: 291 files =  6.0 GB\n",
        "647289876/: 228 files =  7.0 GB\n",
        "651748297/: 336 files =  7.0 GB\n",
        "647244741/: 261 files =  8.0 GB\n",
        "713686035/: 289 files =  8.9 GB\n",
        "647247980/: 299 files =  9.1 GB\n",
        "649052017/: 307 files =  9.4 GB\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-l_U8XU_P2x",
        "colab_type": "text"
      },
      "source": [
        "### Map the 10 testing neuron\n",
        "    \n",
        "The final part of the challenge data set to be mapped is the sub-root directory, `TEST_DATA_SET`, which has 10 neurons laid out like with the training data, except the SWC files are missing i.e. no reconstruction answers given (because, that is what the challenger is supposed to demonstrate: the capability to generate quality SWC files)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bp0cc2ixHx73",
        "colab_type": "code",
        "outputId": "07f3a580-1583-4d9a-bd46-1e3237695fc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "import boto3\n",
        "\n",
        "client = boto3.client('s3',\n",
        "     endpoint_url = 'https://s3.us-west-1.wasabisys.com',\n",
        "     aws_access_key_id = '2G7POM6IZKJ3KLHSC4JB',\n",
        "     aws_secret_access_key = \"0oHD5BXPim7fR1n7zDXpz4YoB7CHAHAvFgzpuJnt\")\n",
        "paginator = client.get_paginator('list_objects')\n",
        "result = paginator.paginate(\n",
        "    Bucket='brightfield-auto-reconstruction-competition', \n",
        "    Prefix=\"TEST_DATA_SET/\", \n",
        "    Delimiter='/')\n",
        "    # See https://stackoverflow.com/a/36992023\n",
        "    # A response can contain CommonPrefixes only if you specify a delimiter. When you do, CommonPrefixes contains all (if there are any) keys between Prefix and the next occurrence of the string specified by delimiter. In effect, CommonPrefixes lists keys that act like subdirectories in the directory specified by Prefix.\n",
        "\n",
        "#for prefix in result.search('CommonPrefixes'):\n",
        "#    print(prefix.get('Prefix'))\n",
        "    \n",
        "testing_neurons = {}\n",
        "\n",
        "# Set up a progress indicator for this slow but not too slow task:\n",
        "progressIndicator = display(progress(0, 10), display_id=True)\n",
        "progressIndicator_count = 0\n",
        "progressIndicator_end = 10\n",
        "\n",
        "for o in result.search('CommonPrefixes'):\n",
        "  progressIndicator_count += 1\n",
        "  progressIndicator.update(progress(progressIndicator_count, progressIndicator_end))\n",
        "  a_prefix = o.get(\"Prefix\")\n",
        "  print(a_prefix)\n",
        "  \n",
        "  # Enumerate all files\n",
        "  # print(\"----------------\")\n",
        "  imagestack_bytes = 0\n",
        "  imagestack = []\n",
        "  swc_key = None\n",
        "  for s3_object in bucket.objects.filter(Prefix = a_prefix):\n",
        "    # print(s3_object.key + \"= \" + str(s3_object.size))\n",
        "    if not s3_object.key.endswith(\".swc\"):\n",
        "      if s3_object.key != a_prefix:\n",
        "        # if == it's the directory itself, not a file in it so ignore\n",
        "        imagestack.append(s3_object.key)\n",
        "        imagestack_bytes += s3_object.size\n",
        "    else:\n",
        "      swc_key = s3_object.key\n",
        "  \n",
        "  # Strip the \"TEST_DATA_SET/\" from Prefix\n",
        "  neuron_id = a_prefix[len(\"TEST_DATA_SET/\"):]\n",
        "  \n",
        "  testing_neurons[neuron_id] = {\"prefix\": a_prefix, \"swc\": swc_key, \"imagestack\": imagestack, \"size\": imagestack_bytes}\n",
        "        \n",
        "print( \"# testing neurons mapped: \" + str(len(testing_neurons)) + \"\\nSorted by size of image stack:\")    \n",
        "    \n",
        "def testing_sizer(x): \n",
        "  return testing_neurons[x][\"size\"]\n",
        "\n",
        "size_sorted_testing_neurons = sorted(testing_neurons, key = testing_sizer) \n",
        "total_bytes_in_testing_dataset = 0\n",
        "\n",
        "for a_neuron_name in size_sorted_testing_neurons:\n",
        "  a_neuron = testing_neurons[a_neuron_name]\n",
        "  fileSize, fileUnits = format_bytes(a_neuron[\"size\"])\n",
        "  total_bytes_in_testing_dataset += a_neuron[\"size\"]\n",
        "  print(a_neuron_name + \": \" + str(len(a_neuron[\"imagestack\"])) + \" files = \" + '{:4.1f}'.format(fileSize) + \" \" + fileUnits )\n",
        "\n",
        "fileSize, fileUnits = format_bytes(total_bytes_in_testing_dataset)\n",
        "print(\"\\nTotal size of testing dataset = \" + '{:4.1f}'.format(fileSize) + \" \" + fileUnits )  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "        <progress\n",
              "            value='10'\n",
              "            max='10',\n",
              "            style='width: 100%'\n",
              "        >\n",
              "            10\n",
              "        </progress>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "TEST_DATA_SET/665856925/\n",
            "TEST_DATA_SET/687730329/\n",
            "TEST_DATA_SET/691311995/\n",
            "TEST_DATA_SET/715953708/\n",
            "TEST_DATA_SET/741428906/\n",
            "TEST_DATA_SET/751017870/\n",
            "TEST_DATA_SET/761936495/\n",
            "TEST_DATA_SET/827413048/\n",
            "TEST_DATA_SET/850675694/\n",
            "TEST_DATA_SET/878858275/\n",
            "# testing neurons mapped: 10\n",
            "Sorted by size of image stack:\n",
            "665856925/: 281 files =  8.6 GB\n",
            "715953708/: 340 files = 10.4 GB\n",
            "751017870/: 465 files = 18.9 GB\n",
            "687730329/: 497 files = 20.3 GB\n",
            "850675694/: 438 files = 23.5 GB\n",
            "827413048/: 424 files = 28.3 GB\n",
            "761936495/: 529 files = 28.5 GB\n",
            "691311995/: 441 files = 29.4 GB\n",
            "741428906/: 591 files = 39.4 GB\n",
            "878858275/: 541 files = 54.0 GB\n",
            "\n",
            "Total size of testing dataset = 261.3 GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jctMs7zbSxby",
        "colab_type": "text"
      },
      "source": [
        "### Notes\n",
        "\n",
        "In many ways this BioImage 2019 brightfield neuron reconstruction challenge dataset is a continuation of the work of The Allen's [BigNeuron project](https://alleninstitute.org/what-we-do/brain-science/news-press/articles/bigneuron-project-launched-advance-3d-reconstructions-neurons) of a few years ago. \n",
        "\n",
        "Here though there is only one type of data: brightfield imaging of biocytin stained neurons. Each neuron's image stack on the order of 10 gigabytes of data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIepIAOiVZhH",
        "colab_type": "text"
      },
      "source": [
        "## Write specimens_manifest.json\n",
        "\n",
        "The file specimens_manifest.json is a logical view of the (~6K) physical files in the dataset. File names within the manifest are relative to the root of the dataset. This file can be used later to provide a clean interface to the library of specimens as well as maintain a per-specimen download cache (useful for notebooks that only process a single specimen because of file system size limitations). Having a download cache is very handy to speed up repeated notebook `Runtime | Run all` because each specimen's data is 6GB to 60GB in size, which is boring to watch happen repeatedly unneccessarily \n",
        "\n",
        "The specimens in the manifest JSON are listed in a flat dictionary, keyed by specimen ID. Filenames in the manifest are relative to the root of the bucket where the specimen dataset resides. \n",
        "\n",
        "(Note by file naming relative to the root of the original source dataset bucket (rather than Colab file system absolute names) folks could also use the specimens_manifest.json file outside the context of Colab. It is a reusable convenience for experimentation on other platforms.)\n",
        "\n",
        "Each specimen has two properties, the local full filename to the .swc file (if any), and the array of full local filenames to the TIFF files in the z-stack.\n",
        "\n",
        "The contents of specimens_manifest.json plus the directory name of root of the local file system cache of files from the dataset is sufficient to resolve to full file names of specimens files, with all the data corralling hassles already taken care of for code that actually does something with these files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WJXi-3Ea7J5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "write(manifest_filename)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}